{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104491,"databundleVersionId":12585144,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":12089999,"sourceType":"datasetVersion","datasetId":7610788},{"sourceId":12090037,"sourceType":"datasetVersion","datasetId":7610814}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T15:50:15.653366Z","iopub.execute_input":"2025-06-07T15:50:15.653749Z","iopub.status.idle":"2025-06-07T15:50:15.667017Z","shell.execute_reply.started":"2025-06-07T15:50:15.653723Z","shell.execute_reply":"2025-06-07T15:50:15.665892Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\n/kaggle/input/summer-analytics-mid-hackathon/hacktrain.csv\n/kaggle/input/hacktest/hacktest.csv\n/kaggle/input/hacktrain-csv/hacktrain.csv\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\ndef load_data(train_path, test_path):\n    \"\"\"Load data with robust column checking\"\"\"\n    train = pd.read_csv(\"/kaggle/input/hacktrain-csv/hacktrain.csv\")\n    test = pd.read_csv(\"/kaggle/input/hacktest/hacktest.csv\")\n    \n    # Check for target column (case insensitive)\n    target_col = None\n    for col in train.columns:\n        if col.lower() == 'class':\n            target_col = col\n            break\n    \n    if target_col is None:\n        raise ValueError(\"Could not find target 'class' column in training data\")\n    \n    # Standardize column names\n    train = train.rename(columns={target_col: 'class'})\n    id_col = 'ID' if 'ID' in train.columns else train.columns[0]\n    train = train.rename(columns={id_col: 'ID'})\n    \n    # Same for test data\n    id_col_test = 'ID' if 'ID' in test.columns else test.columns[0]\n    test = test.rename(columns={id_col_test: 'ID'})\n    \n    return train, test\n\n# Load data\ntrain_data, test_data = load_data('train.csv', 'test.csv')\nprint(\"Training data columns:\", train_data.columns.tolist())\nprint(\"Test data columns:\", test_data.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:17:34.786476Z","iopub.execute_input":"2025-06-07T16:17:34.786843Z","iopub.status.idle":"2025-06-07T16:17:34.857260Z","shell.execute_reply.started":"2025-06-07T16:17:34.786818Z","shell.execute_reply":"2025-06-07T16:17:34.856130Z"}},"outputs":[{"name":"stdout","text":"Training data columns: ['Unnamed: 0', 'ID', 'class', '20150720_N', '20150602_N', '20150517_N', '20150501_N', '20150415_N', '20150330_N', '20150314_N', '20150226_N', '20150210_N', '20150125_N', '20150109_N', '20141117_N', '20141101_N', '20141016_N', '20140930_N', '20140813_N', '20140626_N', '20140610_N', '20140525_N', '20140509_N', '20140423_N', '20140407_N', '20140322_N', '20140218_N', '20140202_N', '20140117_N', '20140101_N']\nTest data columns: ['Unnamed: 0', 'ID', '20150720_N', '20150602_N', '20150517_N', '20150501_N', '20150415_N', '20150330_N', '20150314_N', '20150226_N', '20150210_N', '20150125_N', '20150109_N', '20141117_N', '20141101_N', '20141016_N', '20140930_N', '20140813_N', '20140626_N', '20140610_N', '20140525_N', '20140509_N', '20140423_N', '20140407_N', '20140322_N', '20140218_N', '20140202_N', '20140117_N', '20140101_N']\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"def preprocess_data(df, is_train=True):\n    \"\"\"Robust preprocessing that handles different column formats\"\"\"\n    # Make copy to avoid SettingWithCopyWarning\n    df = df.copy()\n    \n    # Get ID and class columns (case insensitive)\n    id_col = [col for col in df.columns if col.upper() == 'ID']\n    if not id_col:\n        id_col = [col for col in df.columns if 'id' in col.lower()]\n    if not id_col:\n        id_col = [df.columns[0]]  # Fallback to first column\n    \n    if is_train:\n        class_col = [col for col in df.columns if col.lower() == 'class']\n        if not class_col:\n            raise ValueError(\"Could not find 'class' column in training data\")\n    \n    # Extract columns\n    ids = df[id_col[0]]\n    if is_train:\n        classes = df[class_col[0]]\n        ndvi_cols = [col for col in df.columns \n                    if col not in [id_col[0], class_col[0]] \n                    and ('N' in col or 'NDVI' in col.upper())]\n    else:\n        ndvi_cols = [col for col in df.columns \n                    if col != id_col[0] \n                    and ('N' in col or 'NDVI' in col.upper())]\n    \n    # Handle case where no NDVI columns found\n    if not ndvi_cols:\n        ndvi_cols = [col for col in df.columns \n                    if col not in ([id_col[0]] + ([class_col[0]] if is_train else []))]\n    \n    df_ndvi = df[ndvi_cols]\n    \n    # Convert column names to datetime for sorting (more robust)\n    date_columns = []\n    for col in df_ndvi.columns:\n        try:\n            # Try to extract date from column name\n            date_part = col.split('_')[0]\n            pd.to_datetime(date_part)\n            date_columns.append(col)\n        except:\n            continue\n    \n    if not date_columns:\n        # If no dates found, just use original order\n        date_columns_sorted = df_ndvi.columns\n    else:\n        date_columns_sorted = sorted(date_columns, \n                                   key=lambda x: pd.to_datetime(x.split('_')[0]))\n    \n    # Reorder columns chronologically\n    df_ndvi = df_ndvi[date_columns_sorted]\n    \n    # Handle missing values\n    df_ndvi = df_ndvi.interpolate(axis=1, limit_direction='both')\n    imputer = SimpleImputer(strategy='mean')\n    df_imputed = pd.DataFrame(imputer.fit_transform(df_ndvi), \n                             columns=df_ndvi.columns)\n    \n    # Feature engineering\n    features = pd.DataFrame()\n    features['mean_ndvi'] = df_imputed.mean(axis=1)\n    features['std_ndvi'] = df_imputed.std(axis=1)\n    features['min_ndvi'] = df_imputed.min(axis=1)\n    features['max_ndvi'] = df_imputed.max(axis=1)\n    features['range_ndvi'] = features['max_ndvi'] - features['min_ndvi']\n    features['median_ndvi'] = df_imputed.median(axis=1)\n    \n    # Add ID back\n    features['ID'] = ids.values\n    \n    if is_train:\n        features['class'] = classes.values\n    \n    return features\n\n# Preprocess data\ntry:\n    train_processed = preprocess_data(train_data)\n    test_processed = preprocess_data(test_data, is_train=False)\n    print(\"Preprocessing completed successfully!\")\n    print(\"Processed training data shape:\", train_processed.shape)\nexcept Exception as e:\n    print(f\"Error during preprocessing: {str(e)}\")\n    print(\"Available columns in train data:\", train_data.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:17:55.025866Z","iopub.execute_input":"2025-06-07T16:17:55.026233Z","iopub.status.idle":"2025-06-07T16:17:55.495315Z","shell.execute_reply.started":"2025-06-07T16:17:55.026209Z","shell.execute_reply":"2025-06-07T16:17:55.494099Z"}},"outputs":[{"name":"stdout","text":"Preprocessing completed successfully!\nProcessed training data shape: (8000, 8)\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"def train_model(train_df):\n    \"\"\"Train logistic regression model with error handling\"\"\"\n    try:\n        X = train_df.drop(['ID', 'class'], axis=1)\n        y = train_df['class']\n        \n        # Basic validation\n        if len(X) == 0:\n            raise ValueError(\"No features available for training\")\n        if len(y.unique()) < 2:\n            raise ValueError(\"Need at least 2 classes for classification\")\n        \n        pipeline = Pipeline([\n            ('scaler', StandardScaler()),\n            ('classifier', LogisticRegression(\n                multi_class='multinomial',\n                solver='lbfgs',\n                max_iter=1000,\n                class_weight='balanced',\n                random_state=42\n            ))\n        ])\n        \n        pipeline.fit(X, y)\n        return pipeline\n    \n    except Exception as e:\n        print(f\"Error during model training: {str(e)}\")\n        return None\n\n# Train model\nmodel = train_model(train_processed)\nif model is not None:\n    print(\"Model trained successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:18:15.292570Z","iopub.execute_input":"2025-06-07T16:18:15.293900Z","iopub.status.idle":"2025-06-07T16:18:15.749626Z","shell.execute_reply.started":"2025-06-07T16:18:15.293853Z","shell.execute_reply":"2025-06-07T16:18:15.746510Z"}},"outputs":[{"name":"stdout","text":"Model trained successfully!\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"def make_predictions(model, test_df):\n    \"\"\"Generate predictions with checks\"\"\"\n    if model is None:\n        print(\"No model available for predictions\")\n        return None\n    \n    try:\n        X_test = test_df.drop(['ID'], axis=1)\n        test_preds = model.predict(X_test)\n        \n        submission = pd.DataFrame({\n            'ID': test_df['ID'],\n            'class': test_preds\n        })\n        return submission\n    except Exception as e:\n        print(f\"Error during prediction: {str(e)}\")\n        return None\n\n# Generate submission\nsubmission = make_predictions(model, test_processed)\nif submission is not None:\n    submission.to_csv('submission.csv', index=False)\n    print(\"Submission file created successfully!\")\n    print(submission.head())\nelse:\n    print(\"Failed to create submission file\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T16:18:45.994338Z","iopub.execute_input":"2025-06-07T16:18:45.994746Z","iopub.status.idle":"2025-06-07T16:18:46.019099Z","shell.execute_reply.started":"2025-06-07T16:18:45.994719Z","shell.execute_reply":"2025-06-07T16:18:46.017871Z"}},"outputs":[{"name":"stdout","text":"Submission file created successfully!\n   ID    class\n0   1   forest\n1   2  orchard\n2   3  orchard\n3   4  orchard\n4   5  orchard\n","output_type":"stream"}],"execution_count":58}]}